# E-Commerce Price Variation Pipeline

üöÄ **Projeto de Engenharia de Dados End-to-End**

An√°lise de varia√ß√µes de pre√ßos em e-commerce usando o dataset Olist (Brazilian E-Commerce Public Dataset), implementando uma arquitetura Medallion (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold) com Python, pandas e formatos otimizados (Parquet).

---

## üìã Vis√£o Geral

| Item | Descri√ß√£o |
|------|-----------|
| **Objetivo** | Extrair, transformar e agregar dados de e-commerce em KPIs prontos para BI |
| **Dataset** | Olist Brazilian E-Commerce (~100k pedidos, ~1M itens, 72 categorias) |
| **Per√≠odo** | 2016-09 a 2018-10 |
| **Arquitetura** | Medallion (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold) |
| **Tecnologias** | Python 3.8+, pandas, pyarrow (Parquet), Docker (MinIO opcional) |
| **Tempo de Execu√ß√£o** | ~30 seg (amostra) / ~5-10 min (dataset completo) |

---

## üìÇ Estrutura do Reposit√≥rio

```
TrabalhoBigData/
‚îú‚îÄ‚îÄ docs/                              # üìö Documenta√ß√£o completa
‚îÇ   ‚îú‚îÄ‚îÄ DOCUMENTACAO_GERAL.md         # Documenta√ß√£o t√©cnica detalhada (3.500+ linhas)
‚îÇ   ‚îú‚îÄ‚îÄ confluence_export.md           # Pronto para importar no Confluence
‚îÇ   ‚îî‚îÄ‚îÄ confluence_template.md         # Template Confluence
‚îú‚îÄ‚îÄ src/                               # üêç Scripts Python
‚îÇ   ‚îú‚îÄ‚îÄ ingestao.py                   # Etapa 1: Raw ‚Üí Bronze (normaliza√ß√£o)
‚îÇ   ‚îú‚îÄ‚îÄ processamento.py               # Etapa 2: Bronze ‚Üí Silver (transforma√ß√£o)
‚îÇ   ‚îú‚îÄ‚îÄ gold.py                        # Etapa 3: Silver ‚Üí Gold (KPIs)
‚îÇ   ‚îú‚îÄ‚îÄ processamento_fix.py           # Vers√£o otimizada (chunked reading)
‚îÇ   ‚îî‚îÄ‚îÄ gold_fix.py                    # Vers√£o otimizada para datasets grandes
‚îú‚îÄ‚îÄ notebooks/                         # üìä Jupyter Notebooks
‚îÇ   ‚îî‚îÄ‚îÄ 01_eda_olist.ipynb            # An√°lise Explorat√≥ria de Dados (EDA)
‚îú‚îÄ‚îÄ infra/                             # üê≥ Infraestrutura
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yaml            # MinIO (S3-compatible storage, opcional)
‚îú‚îÄ‚îÄ datasets/                          # üì¶ Dados de teste
‚îÇ   ‚îú‚îÄ‚îÄ README.md                      # Guia de dados
‚îÇ   ‚îî‚îÄ‚îÄ sample_data/                   # Amostra para testes r√°pidos
‚îÇ       ‚îú‚îÄ‚îÄ olist_orders_sample.csv
‚îÇ       ‚îú‚îÄ‚îÄ olist_order_items_sample.csv
‚îÇ       ‚îú‚îÄ‚îÄ olist_products_sample.csv
‚îÇ       ‚îú‚îÄ‚îÄ olist_customers_sample.csv
‚îÇ       ‚îî‚îÄ‚îÄ olist_sellers_sample.csv
‚îú‚îÄ‚îÄ Datasets/                          # üèóÔ∏è Data Lake (criado automaticamente)
‚îÇ   ‚îú‚îÄ‚îÄ Source/                        # Dados brutos (Kaggle)
‚îÇ   ‚îú‚îÄ‚îÄ Raw/                           # C√≥pia literal
‚îÇ   ‚îú‚îÄ‚îÄ Bronze/                        # Normalizado (CSV)
‚îÇ   ‚îú‚îÄ‚îÄ Silver/                        # Transformado (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ Gold/                          # KPIs (CSV)
‚îú‚îÄ‚îÄ diagrams/                          # üìê Diagramas
‚îÇ   ‚îî‚îÄ‚îÄ architecture.mmd               # Diagrama Mermaid da arquitetura
‚îú‚îÄ‚îÄ requirements.txt                   # üìã Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                          # Este arquivo
```

---

## üöÄ In√≠cio R√°pido

### Pr√©-requisitos

- Python 3.8+
- pip (gerenciador de pacotes)
- Git
- VS Code (opcional)

### 1Ô∏è‚É£ Clone e Configure

```powershell
# Clone o reposit√≥rio
git clone https://github.com/HenriqueForin0905/TrabalhoBigData.git
cd TrabalhoBigData

# Abra no VS Code (opcional)
code .

# Crie virtual environment
python -m venv venv

# Ative (Windows)
.\venv\Scripts\Activate.ps1
# OU (macOS/Linux)
source venv/bin/activate

# Instale depend√™ncias
python -m pip install --upgrade pip
pip install -r requirements.txt
```

### 2Ô∏è‚É£ Teste R√°pido (30 segundos com amostra)

```powershell
# Copie dados de amostra para Source
cp .\datasets\sample_data\*.csv .\Datasets\Source\

# Execute o pipeline completo
python .\src\ingestao.py        # Raw ‚Üí Bronze
python .\src\processamento.py   # Bronze ‚Üí Silver
python .\src\gold.py             # Silver ‚Üí Gold

# Verifique resultados
ls .\Datasets\Gold\
```

**Resultado esperado:**
- `avg_price_by_category.csv` - Pre√ßo m√©dio por categoria
- `price_variation_by_month.csv` - Varia√ß√£o mensal

### 3Ô∏è‚É£ An√°lise Completa (Dados Kaggle)

```powershell
# Baixe o dataset Olist do Kaggle
# https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce

# Copie os CSVs para Source
cp .\datasets\full_data\olist*.csv .\Datasets\Source\

# Execute o pipeline
python .\src\ingestao.py
python .\src\processamento.py
python .\src\gold.py

# Tempo total: ~5-10 minutos
```

### 4Ô∏è‚É£ An√°lise Explorat√≥ria (EDA)

```powershell
# Abra Jupyter Notebook
jupyter notebook notebooks/01_eda_olist.ipynb

# Explore os dados:
# - Estat√≠sticas descritivas
# - Distribui√ß√µes de pre√ßos
# - An√°lise temporal
# - Visualiza√ß√µes
# - Insights principais
```

---

## üìä Pipeline Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE DE DADOS                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Etapa 1: INGEST√ÉO (ingestao.py)
  Source CSV ‚Üí Read ‚Üí Raw (c√≥pia literal) ‚Üí Bronze (normalizado)
  ‚úì Normaliza nomes de colunas (snake_case)
  ‚úì Preserva dados para auditoria

Etapa 2: PROCESSAMENTO (processamento.py)
  Bronze CSV ‚Üí Load ‚Üí Transforma√ß√£o ‚Üí Silver (Parquet)
  ‚úì Padroniza tipos de dados
  ‚úì Converte datas para datetime
  ‚úì Trata nulos (numeric ‚Üí 0, text ‚Üí NaN, date ‚Üí NaT)
  ‚úì Comprime com Parquet (70-90% menos espa√ßo)

Etapa 3: AGREGA√á√ÉO (gold.py)
  Silver Parquet ‚Üí Load ‚Üí C√°lculos ‚Üí Gold (CSV)
  ‚úì avg_price_by_category (pre√ßo m√©dio por categoria)
  ‚úì price_variation_by_month (varia√ß√£o mensal)
  ‚úì Pronto para BI tools

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Power BI / Metabase / Tableau / Excel (BI Tools)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîë KPIs Gerados

### avg_price_by_category.csv

```csv
product_category_name,avg_price
telefonica,150.45
electronics,120.30
esportes,98.75
...
```

### price_variation_by_month.csv

```csv
month,avg_price,min_price,max_price
2016-09,100.00,10.00,5000.00
2016-10,105.50,12.00,4800.00
...
```

---

## üõ†Ô∏è Integra√ß√£o com BI Tools

### Power BI

1. Abra Power BI Desktop
2. **Home** ‚Üí **Get Data** ‚Üí **Folder**
3. Aponte para `Datasets/Gold/`
4. **Load** e crie visualiza√ß√µes

### Metabase

1. Inicie Metabase (Docker ou local)
2. **Admin Settings** ‚Üí **Databases** ‚Üí **Add Database**
3. Tipo: **File** ‚Üí Aponte para `Datasets/Gold/`
4. **Browse** e crie dashboards

### Google Sheets / Excel

1. **File** ‚Üí **Open** ‚Üí Selecione CSV em `Gold/`
2. Crie gr√°ficos e an√°lises

---

## üìà Dados & Estat√≠sticas

| M√©trica | Valor |
|---------|-------|
| **Total de Pedidos** | ~100.000 |
| **Total de Itens** | ~1.000.000 |
| **Categorias √önicas** | ~72 |
| **Vendedores** | ~3.500 |
| **Clientes** | ~100.000 |
| **Per√≠odo** | 2016-09 a 2018-10 (775 dias) |
| **Pre√ßo M√©dio** | ~R$ 120,77 |
| **Pre√ßo Range** | R$ 0,85 - R$ 13.664,00 |

Para detalhes completos, veja `datasets/README.md`

---

## üìö Documenta√ß√£o

| Arquivo | Conte√∫do |
|---------|----------|
| **DOCUMENTACAO_GERAL.md** | Documenta√ß√£o t√©cnica completa (3.500+ linhas): arquitetura, decis√µes t√©cnicas, troubleshooting, limita√ß√µes |
| **confluence_export.md** | Vers√£o formatada para Confluence (pronta para importar) |
| **notebooks/01_eda_olist.ipynb** | An√°lise explorat√≥ria com visualiza√ß√µes e insights |
| **datasets/README.md** | Guia de dados, esquema, estat√≠sticas |

---

## üîß Comandos √öteis

### Executar Pipeline Completo

```powershell
python .\src\ingestao.py && python .\src\processamento.py && python .\src\gold.py
```

### Verificar Dados em Cada Camada

```powershell
# Raw
ls .\Datasets\Raw\ | Select-Object Name, Length

# Bronze
ls .\Datasets\Bronze\ | Select-Object Name, Length

# Silver
ls .\Datasets\Silver\ | Select-Object Name, Length

# Gold (KPIs)
gc .\Datasets\Gold\avg_price_by_category.csv | head -20
```

### Validar Instala√ß√£o

```powershell
python -c "import pandas; import pyarrow; print('‚úÖ OK')"
```

### Usar com MinIO (Opcional)

```powershell
# Inicie MinIO com Docker
docker-compose -f .\infra\docker-compose.yaml up -d

# Acesse console
# URL: http://localhost:9001
# User: minioadmin
# Password: minioadmin

# Pare o servi√ßo
docker-compose -f .\infra\docker-compose.yaml down
```

---

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Ajustar Diret√≥rios

```powershell
# Usar diret√≥rios customizados
python .\src\ingestao.py --source "C:\path\to\data" --datasets-dir "C:\path\to\Datasets"

python .\src\processamento.py --datasets-dir "C:\path\to\Datasets"

python .\src\gold.py --datasets-dir "C:\path\to\Datasets"
```

### Agendar Execu√ß√£o Autom√°tica (Windows Task Scheduler)

1. Crie arquivo `run_pipeline.bat`:
   ```batch
   @echo off
   cd C:\Users\...\TrabalhoBigData
   .\venv\Scripts\Activate.ps1
   python .\src\ingestao.py && python .\src\processamento.py && python .\src\gold.py
   echo Pipeline executed at %date% %time% >> pipeline.log
   ```

2. Abra Task Scheduler e agende para rodar diariamente

### Escalar para PySpark

Para datasets > 1GB, migre para PySpark:

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Pipeline").getOrCreate()
df = spark.read.csv("source.csv")
# Processamento distribu√≠do
```

---

## üêõ Troubleshooting

### Erro: "ModuleNotFoundError: No module named 'pandas'"

```powershell
pip install pandas>=1.3
```

### Erro: "No such file or directory: 'Datasets/Source'"

```powershell
mkdir .\Datasets\Source
# Copie os CSVs aqui
```

### Erro: "OutOfMemory" em datasets grandes

```powershell
# Use vers√£o otimizada
python .\src\processamento_fix.py
```

Para mais solu√ß√µes, veja se√ß√£o "Troubleshooting" em `docs/DOCUMENTACAO_GERAL.md`

---

## üéØ Pr√≥ximos Passos

- [ ] Baixe dados completos do Kaggle
- [ ] Execute pipeline completo
- [ ] Crie dashboard em Power BI ou Metabase
- [ ] Estenda com KPIs customizados
- [ ] Implemente orchestra√ß√£o (Airflow)
- [ ] Configure backup/DR

---

## üìñ Refer√™ncias

- **Dataset Olist:** https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
- **Pandas Docs:** https://pandas.pydata.org/docs/
- **Parquet Format:** https://parquet.apache.org/
- **Medallion Architecture:** [Delta Lake](https://delta.io/)

---

## üìù Licen√ßa

Este projeto est√° licenciado sob MIT License. O dataset Olist √© de dom√≠nio p√∫blico.

---

**√öltima Atualiza√ß√£o:** Dezembro 2025  
**Mantido por:** Projeto TrabalhoBigData  
**Contato:** [GitHub Issues](https://github.com/HenriqueForin0905/TrabalhoBigData/issues)
