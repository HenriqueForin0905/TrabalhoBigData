# Scripts Python - Guia Completo

Este diretÃ³rio contÃ©m os scripts principais do pipeline de dados.

## ðŸ“‹ VisÃ£o Geral

```
src/
â”œâ”€â”€ ingestao.py              # Etapa 1: Source â†’ Raw â†’ Bronze
â”œâ”€â”€ processamento.py         # Etapa 2: Bronze â†’ Silver
â”œâ”€â”€ gold.py                  # Etapa 3: Silver â†’ Gold
â”œâ”€â”€ processamento_fix.py     # VersÃ£o otimizada (chunked reading)
â”œâ”€â”€ gold_fix.py              # VersÃ£o otimizada para grandes datasets
â””â”€â”€ README.md                # Este arquivo
```

---

## ðŸ”„ Pipeline Scripts

### 1. ingestao.py - Leitura e NormalizaÃ§Ã£o

**Responsabilidade**: Ler CSVs brutos e normalizar nomes de colunas

**Fluxo**:
```
Source CSV â†’ pandas.read_csv() â†’ Raw (cÃ³pia literal) â†’ Normalizar colunas â†’ Bronze
```

**SaÃ­da**:
- `Datasets/Raw/*.csv` - CÃ³pia exata dos CSVs originais
- `Datasets/Bronze/*.csv` - CSVs com colunas normalizadas

**TransformaÃ§Ãµes**:
1. Strip whitespace
2. Converter para lowercase
3. EspaÃ§os â†’ underscores
4. Remove caracteres especiais (mantÃ©m alfanumÃ©ricos + _)
5. Remove mÃºltiplos underscores consecutivos
6. Remove underscores leading/trailing

**Exemplo**:
```
"Product Category Name" â†’ "product_category_name"
"Order Approved AT!" â†’ "order_approved_at"
"Price (BRL)" â†’ "price_brl"
```

**ExecuÃ§Ã£o**:
```powershell
# Uso bÃ¡sico
python .\src\ingestao.py

# Com diretÃ³rios customizados
python .\src\ingestao.py --source "C:\data" --datasets-dir "C:\Datasets"

# Ver ajuda
python .\src\ingestao.py --help
```

**Argumentos**:
- `--source, -s`: DiretÃ³rio com CSVs originais (default: `Datasets/Source`)
- `--datasets-dir, -d`: Raiz do data lake (default: diretÃ³rio pai do script)

**Tratamento de Erros**:
- Arquivo nÃ£o existe â†’ Warning, continua com prÃ³ximo
- Erro ao ler CSV â†’ Log exception, continua
- Erro ao escrever â†’ Log exception, continua

---

### 2. processamento.py - TransformaÃ§Ã£o e PadronizaÃ§Ã£o

**Responsabilidade**: Aplicar transformaÃ§Ãµes e padronizar tipos de dados

**Fluxo**:
```
Bronze CSV â†’ Load â†’ Standardize Types â†’ Handle Nulls â†’ Silver Parquet
```

**SaÃ­da**:
- `Datasets/Silver/*.parquet` - Dados transformados em formato Parquet

**TransformaÃ§Ãµes**:

1. **Normalizar Colunas**: lowercase, trim
2. **Parse Dates**:
   - Detecta: colunas com 'date', '_dt', '_date' no nome
   - Converte para datetime64
   - Erros â†’ NaT (Not a Time)
3. **Fill Nulls**:
   - Colunas numÃ©ricas: NaN â†’ 0
   - Colunas texto: deixar NaN (NULL)
   - Colunas date: deixar NaT

4. **Salvar em Parquet**:
   - CompressÃ£o automÃ¡tica (snappy, gzip, ou uncompressed)
   - Preserva tipos de dados nativos
   - ~70-90% mais compacto que CSV

**Exemplo**:
```
Entrada (Bronze):
order_id | order_purchase_timestamp | price
1        | 2016-09-04 21:15:13     | 50.3
2        | NULL                     | NULL

SaÃ­da (Silver - Parquet):
order_id | order_purchase_timestamp | price
1        | 2016-09-04 21:15:13     | 50.3
2        | NaT                      | 0.0
```

**ExecuÃ§Ã£o**:
```powershell
# Uso bÃ¡sico
python .\src\processamento.py

# Com diretÃ³rio customizado
python .\src\processamento.py --datasets-dir "C:\Datasets"

# VersÃ£o otimizada (memÃ³ria)
python .\src\processamento_fix.py  # chunked reading
```

**Performance**:
- 100k linhas: ~10-30 segundos
- 1M linhas: ~1-2 minutos
- 10M+ linhas: Usar processamento_fix.py

---

### 3. gold.py - AgregaÃ§Ã£o e KPIs

**Responsabilidade**: Calcular mÃ©tricas prontas para BI

**Fluxo**:
```
Silver Parquet â†’ Load â†’ Join & Aggregate â†’ Gold CSV (KPIs)
```

**SaÃ­da**:
- `Datasets/Gold/avg_price_by_category.csv`
- `Datasets/Gold/price_variation_by_month.csv`

**KPIs Calculados**:

#### KPI 1: avg_price_by_category

```csv
product_category_name,avg_price
telefonica,150.45
electronics,120.30
esportes,98.75
```

**SQL Equivalente**:
```sql
SELECT 
    p.product_category_name,
    AVG(oi.price) as avg_price
FROM order_items oi
JOIN products p ON oi.product_id = p.product_id
GROUP BY p.product_category_name
ORDER BY avg_price DESC
```

#### KPI 2: price_variation_by_month

```csv
month,avg_price,min_price,max_price
2016-09,100.00,10.00,5000.00
2016-10,105.50,12.00,4800.00
```

**SQL Equivalente**:
```sql
SELECT 
    DATE_TRUNC('month', oi.order_approved_at) as month,
    AVG(oi.price) as avg_price,
    MIN(oi.price) as min_price,
    MAX(oi.price) as max_price
FROM order_items oi
GROUP BY DATE_TRUNC('month', oi.order_approved_at)
ORDER BY month
```

**ExecuÃ§Ã£o**:
```powershell
# Uso bÃ¡sico
python .\src\gold.py

# Com diretÃ³rio customizado
python .\src\gold.py --datasets-dir "C:\Datasets"

# VersÃ£o otimizada
python .\src\gold_fix.py
```

---

## ðŸš€ Executar Pipeline Completo

### Sequencial

```powershell
# Modo 1: Um por um
python .\src\ingestao.py
python .\src\processamento.py
python .\src\gold.py

# Modo 2: Em cadeia (&&)
python .\src\ingestao.py && python .\src\processamento.py && python .\src\gold.py

# Modo 3: Com script batch (Windows)
# Crie run_pipeline.bat:
@echo off
python .\src\ingestao.py
python .\src\processamento.py
python .\src\gold.py
echo Done!
```

### Paralelo (PySpark)

Para grandes datasets, use versÃµes Spark:

```python
# Modificar scripts para usar PySpark
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("EcommercePipeline") \
    .getOrCreate()

# Leitura paralela
df = spark.read.csv("path/to/file.csv", header=True)

# Processamento distribuÃ­do
df.coalesce(4).write.parquet("output_path")
```

---

## ðŸ” Troubleshooting

### Problema: ModuleNotFoundError

```powershell
# SoluÃ§Ã£o
pip install pandas pyarrow fastparquet
```

### Problema: OutOfMemory

```powershell
# Use versÃµes otimizadas
python .\src\processamento_fix.py  # chunked reading
python .\src\gold_fix.py            # memory efficient
```

### Problema: Arquivo nÃ£o encontrado

```powershell
# Verifique estrutura
ls .\Datasets\Source\
ls .\Datasets\Bronze\
ls .\Datasets\Silver\

# Use argumentos customizados
python .\src\ingestao.py --source "C:\path\to\data"
```

### Problema: Parquet nÃ£o pode ser lido

```powershell
# Instale/atualize pyarrow
pip install --upgrade pyarrow fastparquet
```

---

## ðŸ“Š Logging e Debugging

### Aumentar Verbosidade

Modifique nÃ­vel de log:

```python
# PadrÃ£o: INFO
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# Debug (verbose)
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(name)s %(levelname)s %(message)s')
```

### Salvar Logs em Arquivo

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s',
    handlers=[
        logging.FileHandler('pipeline.log'),
        logging.StreamHandler()
    ]
)
```

---

## ðŸ§ª Testes

### Teste UnitÃ¡rio

```python
# tests/test_ingestao.py
import pytest
from src.ingestao import _norm_column

def test_norm_column():
    assert _norm_column("Product Name") == "product_name"
    assert _norm_column("Price (BRL)") == "price_brl"
    assert _norm_column("  Spaced  ") == "spaced"

# Rodar testes
pytest tests/test_ingestao.py -v
```

### Validar Dados

```python
# validate.py
import pandas as pd

def validate_pipeline():
    # Load dados
    orders = pd.read_parquet('Datasets/Silver/olist_orders.parquet')
    
    # ValidaÃ§Ãµes
    assert len(orders) > 0, "Orders vazio"
    assert orders['order_id'].is_unique, "Duplicatas em order_id"
    assert orders['order_purchase_timestamp'].notna().sum() > 0.95 * len(orders), "Muitos nulos"
    
    print("âœ… ValidaÃ§Ã£o passou")

validate_pipeline()
```

---

## ðŸ“ˆ Performance

### Benchmarks

| Etapa | Dataset | Tempo | MemÃ³ria |
|-------|---------|-------|---------|
| IngestÃ£o | 100k | 5s | 100MB |
| Processamento | 100k | 10s | 200MB |
| Gold | 100k | 3s | 100MB |
| **Total** | 100k | **18s** | **400MB** |
| | | | |
| IngestÃ£o | 1M | 30s | 800MB |
| Processamento | 1M | 60s | 1.2GB |
| Gold | 1M | 15s | 500MB |
| **Total** | 1M | **2min** | **2.5GB** |

### OtimizaÃ§Ãµes

1. **Usar dtypes explÃ­citos**:
   ```python
   dtype_dict = {
       'price': 'float32',
       'product_id': 'string',
       'order_id': 'string'
   }
   df = pd.read_csv('file.csv', dtype=dtype_dict)
   ```

2. **Chunked Reading**:
   ```python
   for chunk in pd.read_csv('large_file.csv', chunksize=10000):
       process(chunk)
   ```

3. **Usar Parquet**:
   - CompressÃ£o automÃ¡tica
   - Leitura seletiva de colunas
   - Tipos nativos preservados

4. **ParalelizaÃ§Ã£o** (PySpark):
   - Distribuir processamento entre cores
   - Escalar para cluster

---

## ðŸ”— IntegraÃ§Ã£o com BI

### Power BI

```powershell
# Copiar CSVs para local acessÃ­vel
cp .\Datasets\Gold\*.csv .\Gold_Export\

# Power BI â†’ Get Data â†’ Folder â†’ Apontar para .\Gold_Export\
```

### Metabase

```powershell
# Metabase â†’ Admin â†’ Databases â†’ File â†’ Apontar para .\Datasets\Gold\
```

### Python + Plotly

```python
import pandas as pd
import plotly.express as px

gold = pd.read_csv('Datasets/Gold/avg_price_by_category.csv')
fig = px.bar(gold, x='product_category_name', y='avg_price')
fig.show()
```

---

## ðŸ“– ReferÃªncias

- **Pandas**: https://pandas.pydata.org/docs/
- **PyArrow**: https://arrow.apache.org/docs/python/
- **PySpark**: https://spark.apache.org/docs/latest/api/python/
- **Data Engineering**: https://www.oreilly.com/library/view/fundamentals-of-data/

---

**Ãšltima AtualizaÃ§Ã£o:** Dezembro 2025
