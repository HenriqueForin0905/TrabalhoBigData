# Documenta√ß√£o Geral - E-Commerce Price Variation Pipeline

**Vers√£o:** 1.0  
**Data:** Dezembro 2025  
**Autor:** Projeto TrabalhoBigData  

---

## √çndice

1. [Descri√ß√£o do Problema](#descri√ß√£o-do-problema)
2. [Objetivos do Sistema](#objetivos-do-sistema)
3. [Escopo da Solu√ß√£o](#escopo-da-solu√ß√£o)
4. [Arquitetura Completa](#arquitetura-completa)
5. [Ferramentas e Tecnologias](#ferramentas-e-tecnologias)
6. [Decis√µes T√©cnicas](#decis√µes-t√©cnicas)
7. [Guia de Execu√ß√£o](#guia-de-execu√ß√£o)
8. [Guia de Depend√™ncias](#guia-de-depend√™ncias)
9. [Descri√ß√£o dos Dados](#descri√ß√£o-dos-dados)
10. [Pontos de Falha e Limita√ß√µes](#pontos-de-falha-e-limita√ß√µes)

---

## Descri√ß√£o do Problema

### Contexto

O projeto aborda a **an√°lise de varia√ß√µes de pre√ßos em e-commerce** utilizando dados hist√≥ricos do setor de vendas online brasileiro. O dataset Olist (Brazilian E-Commerce Public Dataset) cont√©m informa√ß√µes reais de transa√ß√µes de um marketplace multi-vendedor.

### Desafios

1. **Volume de Dados**: Mill√µes de registros de pedidos, produtos, vendedores e pre√ßos requerem processamento eficiente
2. **Heterogeneidade de Dados**: Dados brutos com inconsist√™ncias de nomenclatura, tipos e valores faltantes
3. **An√°lise Temporal**: Identificar padr√µes de pre√ßo ao longo do tempo √© complexo sem estrutura√ß√£o adequada
4. **Escalabilidade**: A solu√ß√£o deve permitir adicionar novas fontes de dados sem refatora√ß√£o completa

### Problema Central

> Como estruturar e processar dados de e-commerce para extrair insights sobre varia√ß√£o de pre√ßos em categorias de produtos e per√≠odos temporais, permitindo que ferramentas de BI (Business Intelligence) fa√ßam an√°lises autoatendidas?

---

## Objetivos do Sistema

### Objetivos Prim√°rios

1. **Ingest√£o Automatizada**: Carregar dados de m√∫ltiplas fontes CSV com normaliza√ß√£o b√°sica
2. **Processamento em Camadas**: Implementar arquitetura de data lake com raw ‚Üí bronze ‚Üí silver ‚Üí gold
3. **Transforma√ß√£o de Qualidade**: Padronizar tipos de dados, tratar nulos e normalizar nomenclaturas
4. **Gera√ß√£o de KPIs**: Produzir m√©tricas prontas para visualiza√ß√£o (pre√ßo m√©dio por categoria, varia√ß√£o mensal)
5. **Integra√ß√£o com BI**: Exportar dados estruturados (Parquet/CSV) para ferramentas como Power BI ou Metabase

### Objetivos Secund√°rios

- Fornecer rastreabilidade completa (raw ‚Üí gold)
- Permitir reprocessamento incremental
- Documentar esquemas de dados em cada camada
- Facilitar manuten√ß√£o e extens√£o do pipeline

### Justificativa T√©cnica

A arquitetura em camadas (Medallion Architecture) oferece:
- **Isolamento**: Cada etapa tem responsabilidade √∫nica
- **Rastreabilidade**: Dados brutos preservados para auditoria
- **Flexibilidade**: Novas transforma√ß√µes adicionadas sem afetar existentes
- **Performance**: Formatos otimizados (Parquet) em camadas superiores
- **Governan√ßa**: Controle expl√≠cito sobre qualidade dos dados em cada n√≠vel

---

## Escopo da Solu√ß√£o

### Inclu√≠do

| Item | Descri√ß√£o |
|------|-----------|
| **Ingest√£o (Raw)** | Leitura de CSVs da pasta Source; c√≥pia sem altera√ß√£o para Raw |
| **Normaliza√ß√£o (Bronze)** | Limpeza de nomes de colunas (snake_case, alfanum√©ricos) |
| **Processamento (Silver)** | Convers√£o de tipos, tratamento de nulos, valida√ß√£o b√°sica |
| **Agrega√ß√£o (Gold)** | KPIs: pre√ßo m√©dio por categoria, varia√ß√£o mensal de pre√ßos |
| **Exporta√ß√£o** | Formatos CSV (Gold) e Parquet (Silver) para BI tools |
| **Logging** | Rastreamento de execu√ß√£o e erros em cada etapa |
| **Infraestrutura Opcional** | Docker Compose com MinIO para armazenamento distribu√≠do |

### N√£o Inclu√≠do

| Item | Motivo |
|------|--------|
| **Orchestra√ß√£o (Airflow, Dagster)** | Escopo simplificado; execu√ß√£o manual ou cron jobs |
| **Data Warehouse (Snowflake, BigQuery)** | Foco em estrutura local; extens√≠vel a cloud |
| **ML/Previs√£o** | Foco em an√°lise descritiva; n√£o preditiva |
| **API REST** | Dados consumidos diretamente de arquivos |
| **Testes Automatizados** | Escopo m√≠nimo; testes manuais esperados |
| **CI/CD Pipeline** | N√£o configurado; pode ser adicionado |
| **Replica√ß√£o de Dados** | Opera√ß√£o unit√°ria, n√£o cont√≠nua |
| **Data Quality Checks Complexos** | Valida√ß√µes b√°sicas apenas |

---

## Arquitetura Completa

### 1. Arquitetura Geral (Medallion Architecture)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     E-COMMERCE PRICE VARIATION PIPELINE            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SOURCE    ‚îÇ     ‚îÇ      RAW LAYER   ‚îÇ     ‚îÇ   BRONZE LAYER   ‚îÇ
‚îÇ  (External) ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (Dados Brutos)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (Normalizado)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚ñ≤                       ‚îÇ                        ‚îÇ
     ‚îÇ                       ‚îÇ                        ‚îÇ
  CSV Files             CSV Format             CSV Format
  (Olist Dataset)       No Changes            Column Names:
                        Preserva√ß√£o            - snake_case
                        Completa              - only alphanumeric
                                             - normalized spacing

        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  SILVER LAYER    ‚îÇ     ‚îÇ   GOLD LAYER     ‚îÇ
        ‚îÇ (Processado)     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (KPIs/Analytics) ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                           ‚îÇ
        Parquet Format              CSV Format
        - Type Parsing              - avg_price_by_category
        - Null Handling            - price_variation_by_month
        - Date Conversion          (Ready for BI)
```

### 2. Estrutura de Diret√≥rios

```
TrabalhoBigData/
‚îú‚îÄ‚îÄ src/                              # Scripts Python
‚îÇ   ‚îú‚îÄ‚îÄ ingestao.py                  # Raw ‚Üí Bronze
‚îÇ   ‚îú‚îÄ‚îÄ processamento.py              # Bronze ‚Üí Silver
‚îÇ   ‚îú‚îÄ‚îÄ gold.py                       # Silver ‚Üí Gold
‚îÇ   ‚îú‚îÄ‚îÄ processamento_fix.py          # Vers√£o alternativa
‚îÇ   ‚îú‚îÄ‚îÄ gold_fix.py                   # Vers√£o alternativa
‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/                 # Cache Python
‚îú‚îÄ‚îÄ Datasets/                         # Data Lake
‚îÇ   ‚îú‚îÄ‚îÄ Source/                      # Dados externos (Olist)
‚îÇ   ‚îú‚îÄ‚îÄ Raw/                         # C√≥pia literal
‚îÇ   ‚îú‚îÄ‚îÄ Bronze/                      # Normalizado
‚îÇ   ‚îú‚îÄ‚îÄ Silver/                      # Processado (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ Gold/                        # KPIs (CSV)
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yaml          # MinIO (opcional)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ DOCUMENTACAO_GERAL.md        # Este arquivo
‚îÇ   ‚îî‚îÄ‚îÄ confluence_template.md        # Template Confluence
‚îú‚îÄ‚îÄ diagrams/
‚îÇ   ‚îî‚îÄ‚îÄ architecture.mmd              # Diagrama Mermaid
‚îú‚îÄ‚îÄ requirements.txt                  # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                         # Quick start
```

### 3. Fluxo de Dados Detalhado

#### **Etapa 1: Ingest√£o (ingestao.py)**

```
Input: Datasets/Source/*.csv (Olist Dataset)
  ‚îú‚îÄ olist_customers_dataset.csv
  ‚îú‚îÄ olist_orders_dataset.csv
  ‚îú‚îÄ olist_order_items_dataset.csv
  ‚îú‚îÄ olist_products_dataset.csv
  ‚îî‚îÄ olist_sellers_dataset.csv

Processing:
  1. Read CSV file ‚Üí pandas DataFrame
  2. Save unmodified ‚Üí Datasets/Raw/
  3. Normalize column names:
     - Strip whitespace
     - Convert to lowercase
     - Replace spaces with underscores
     - Remove special characters (keep only alphanumeric + _)
     - Remove leading/trailing underscores
  4. Save normalized ‚Üí Datasets/Bronze/

Output: Datasets/Bronze/*.csv (Normalized CSVs)
```

**Fun√ß√£o Principal: `ingest_csv()`**

```python
# Normaliza√ß√£o de colunas
def _norm_column(col: str) -> str:
    col = str(col).strip().lower()
    col = re.sub(r"\s+", "_", col)                    # espa√ßos ‚Üí _
    col = re.sub(r"[^0-9a-zA-Z_]+", "_", col)       # caracteres especiais
    col = re.sub(r"_+", "_", col)                     # m√∫ltiplos _ ‚Üí _
    return col.strip("_")
```

#### **Etapa 2: Processamento (processamento.py)**

```
Input: Datasets/Bronze/*.csv (Normalized CSVs)

Processing:
  1. Load each Bronze CSV
  2. Standardize column names (lowercase, trim)
  3. Parse dates:
     - Detecta colunas com 'date', '_dt', '_date'
     - Converte para datetime (coerce errors ‚Üí NaT)
  4. Fill nulls for numeric columns:
     - Select numeric columns
     - Replace NaN with 0
  5. Save to Parquet (compression, efficient storage)

Output: Datasets/Silver/*.parquet (Processed Data)

Exemplo de convers√£o:
  Entrada (Bronze):
    order_id | order_purchase_timestamp | order_value | ...
    1        | 2016-09-04 21:15:13     | 50.3       | ...
    2        | NULL                     | NULL       | ...

  Sa√≠da (Silver):
    order_id | order_purchase_timestamp | order_value | ...
    1        | 2016-09-04 21:15:13     | 50.3       | ...
    2        | NaT                      | 0          | ...
    
  (Parquet format, compressed)
```

#### **Etapa 3: Agrega√ß√£o Gold (gold.py)**

```
Input: Datasets/Silver/*.parquet (Processed Data)

Processing:
  1. Load all Silver Parquet files as DataFrames
  2. Generate KPI 1: avg_price_by_category
     - Join order_items (price) + products (category)
     - GROUP BY product_category_name
     - Calculate MEAN(price)
     - Output: category | avg_price
     
  3. Generate KPI 2: price_variation_over_time
     - Extract order_approved_at month
     - GROUP BY month
     - Calculate MEAN, MIN, MAX of price
     - Output: month | avg_price | min_price | max_price

Output: Datasets/Gold/*.csv (Ready for BI)
```

### 4. Fluxo de Execu√ß√£o Pipeline

```mermaid
graph TD
    A["Start<br/>python src/ingestao.py"] --> B["Read Source CSVs"]
    B --> C["Save to Raw<br/>No changes"]
    C --> D["Normalize Columns<br/>Bronze"]
    D --> E["Save to Bronze<br/>CSV format"]
    
    E --> F["Start<br/>python src/processamento.py"]
    F --> G["Load Bronze CSVs"]
    G --> H["Parse Dates<br/>Clean Nulls"]
    H --> I["Save to Silver<br/>Parquet format"]
    
    I --> J["Start<br/>python src/gold.py"]
    J --> K["Load Silver Parquets"]
    K --> L["Generate KPIs<br/>Aggregations"]
    L --> M["Save to Gold<br/>CSV format"]
    
    M --> N["üìä Ready for BI<br/>Power BI / Metabase"]
    
    style A fill:#e1f5ff
    style N fill:#c8e6c9
```

---

## Ferramentas e Tecnologias

### Linguagem & Runtime

| Tecnologia | Vers√£o | Prop√≥sito |
|-----------|--------|----------|
| **Python** | 3.8+ | Linguagem principal para processamento |
| **pip** | Latest | Gerenciador de pacotes Python |

### Depend√™ncias de Processamento

| Biblioteca | Vers√£o | Prop√≥sito |
|-----------|--------|----------|
| **pandas** | ‚â•1.3 | Manipula√ß√£o de DataFrames, leitura/escrita CSV |
| **pyarrow** | ‚â•8.0 | Engine para Parquet, integra√ß√£o com pandas |
| **fastparquet** | ‚â•0.8 | Engine alternativa para Parquet |
| **pyspark** | ‚â•3.2 | Opcional: processamento distribu√≠do em escala |

### Armazenamento (Opcional)

| Ferramenta | Vers√£o | Prop√≥sito |
|-----------|--------|----------|
| **MinIO** | latest | S3-compatible object storage (Docker) |
| **Docker** | Latest | Containeriza√ß√£o (docker-compose) |

### Infraestrutura & Deployment

| Ferramenta | Prop√≥sito |
|-----------|----------|
| **File System** | Armazenamento local (Datasets/) |
| **Docker Compose** | Orquestra√ß√£o de containers (MinIO) |

### Ferramentas de BI (Downstream)

- **Power BI**: Importa√ß√£o de CSV/Parquet do Gold
- **Metabase**: Query sobre CSVs em Gold
- **Tableau**: Integra√ß√£o com dados estruturados
- **Google Sheets / Excel**: Importa√ß√£o direta de CSVs

### Vers√µes Espec√≠ficas em requirements.txt

```
pandas>=1.3
pyarrow>=8.0
fastparquet>=0.8
pyspark>=3.2  # optional
```

---

## Decis√µes T√©cnicas

### 1. Arquitetura Medallion (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold)

**Alternativas Consideradas:**
- ‚ùå Pipeline √∫nico (Raw ‚Üí Gold): Perda de rastreabilidade, dif√≠cil auditoria
- ‚ùå Data Warehouse centralizado (Snowflake): Custo $$, overkill para escopo inicial
- ‚úÖ **Medallion Local**: Simplicidade, rastreabilidade, extens√≠vel

**Trade-offs:**
- ‚úÖ Preserva dados brutos para reprocessamento
- ‚úÖ Cada camada tem responsabilidade clara
- ‚ùå Uso de disco duplicado (Raw + Bronze similar)
- ‚ùå Processamento sequencial (n√£o paralelo)

### 2. Formatos de Arquivo por Camada

| Camada | Formato | Justificativa |
|--------|---------|---------------|
| **Raw** | CSV | Preserva√ß√£o literal; simples, universal |
| **Bronze** | CSV | Compat√≠vel com ferramentas padr√£o |
| **Silver** | **Parquet** | Compress√£o, tipos nativos, efici√™ncia |
| **Gold** | CSV | Universalidade para BI tools |

**Por que Parquet em Silver?**
- Compress√£o autom√°tica (reduz 70-90% do tamanho)
- Tipos de dados preservados (date, int, float nativo)
- Query push-down (leitura seletiva de colunas)
- Standard em data lakes (Spark, Hadoop, etc.)

### 3. Normaliza√ß√£o de Colunas em Bronze

**Problema:** Dados Olist t√™m nomes inconsistentes
- "Product Category Name", "PRODUCT_CATEGORY_NAME", "product category name"

**Solu√ß√£o Adotada:**
```python
def _norm_column(col: str) -> str:
    col = str(col).strip().lower()              # lowercase
    col = re.sub(r"\s+", "_", col)             # espa√ßos ‚Üí _
    col = re.sub(r"[^0-9a-zA-Z_]+", "_", col) # especiais ‚Üí _
    col = re.sub(r"_+", "_", col)              # _ m√∫ltiplos ‚Üí _
    return col.strip("_")
```

**Trade-offs:**
- ‚úÖ Consist√™ncia garantida
- ‚úÖ SQL-friendly (snake_case)
- ‚ùå Perde alguns caracteres (√© ‚Üí e, √ß ‚Üí c)
- ‚ùå Colunas com s√≠mbolos podem se confundir

### 4. Tratamento de Nulos em Silver

**Abordagem:**
- Colunas num√©ricas: fill NaN com 0
- Colunas texto: deixar NaN (NULL)
- Datas: coerce invalid ‚Üí NaT

**Justificativa:**
- 0 √© seguro para agrega√ß√µes (SUM, AVG)
- NaT preserva sem√¢ntica de dados faltantes
- Evita correla√ß√µes falsas

**Limita√ß√£o:** Nem sempre 0 √© apropriado (ex: pre√ßo 0 ‚â† nulo)

### 5. KPIs em Gold

**Gerados:**

1. **avg_price_by_category**
   ```
   product_category_name | avg_price
   electronics           | 150.45
   books                 | 25.30
   ```

2. **price_variation_by_month**
   ```
   month  | avg_price | min_price | max_price
   2016-09 | 100.00   | 10.00     | 5000.00
   2016-10 | 105.50   | 15.00     | 4500.00
   ```

**Por que estes KPIs?**
- Respondentes a perguntas de neg√≥cio
- F√°cil visualiza√ß√£o em BI
- Agrega√ß√µes simples (GROUP BY, AVG)

### 6. Processamento Sequencial vs. Paralelo

**Decis√£o:** Sequencial (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold)

**Trade-offs:**
- ‚úÖ Simples de entender e debugar
- ‚úÖ Usa menos mem√≥ria
- ‚ùå Mais lento em datasets grandes (milh√µes de linhas)
- ‚ùå N√£o aproveita m√∫ltiplos cores

**Como Escalar:**
- Substituir pandas ‚Üí PySpark para paralelismo
- Usar Airflow/Prefect para orquestra√ß√£o
- Cloud: Databricks, Google Cloud Dataflow

---

## Guia de Execu√ß√£o

### Pr√©-requisitos

- **Sistema Operacional**: Windows / macOS / Linux
- **Python**: 3.8+ instalado
- **pip**: Gerenciador de pacotes Python
- **Git**: Para clonar reposit√≥rio
- **VS Code** (Opcional): Para editar/debugar c√≥digo

### 1. Clone o Reposit√≥rio

```powershell
# Clone
git clone https://github.com/HenriqueForin0905/TrabalhoBigData.git

# Entre no diret√≥rio
cd TrabalhoBigData

# Abra no VS Code (opcional)
code .
```

### 2. Configure o Ambiente

```powershell
# Crie um Virtual Environment (recomendado)
python -m venv venv

# Ative o Virtual Environment
# Windows:
.\venv\Scripts\Activate.ps1

# macOS/Linux:
source venv/bin/activate

# Instale depend√™ncias
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

**Verificar instala√ß√£o:**
```powershell
python -c "import pandas; import pyarrow; print('OK')"
```

### 3. Prepare os Dados Fonte (Source)

Obtenha o dataset Olist:

```powershell
# Op√ß√£o 1: Download manual
# Visite: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
# Extraia em: Datasets/Source/

# Op√ß√£o 2: Criar estrutura de exemplo
mkdir -p Datasets/Source
# Coloque CSVs aqui: olist_*.csv
```

**Arquivos esperados em Datasets/Source/:**
```
olist_customers_dataset.csv
olist_orders_dataset.csv
olist_order_items_dataset.csv
olist_products_dataset.csv
olist_sellers_dataset.csv
```

### 4. Execute o Pipeline Completo

#### **Etapa 1: Ingest√£o (Raw ‚Üí Bronze)**

```powershell
python .\src\ingestao.py
```

**Sa√≠da esperada:**
```
2025-12-10 14:30:45 INFO Using source dir: C:\...\Datasets\Source
2025-12-10 14:30:46 INFO Ingesting file: olist_customers_dataset.csv
2025-12-10 14:30:46 INFO Saved RAW: C:\...\Datasets\Raw\olist_customers_dataset.csv
2025-12-10 14:30:46 INFO Saved BRONZE: C:\...\Datasets\Bronze\olist_customers_dataset.csv
...
```

**Diret√≥rios criados:**
- `Datasets/Raw/` - C√≥pia literal
- `Datasets/Bronze/` - Normalizado

#### **Etapa 2: Processamento (Bronze ‚Üí Silver)**

```powershell
python .\src\processamento.py
```

**Sa√≠da esperada:**
```
2025-12-10 14:31:00 INFO Processing Bronze file: C:\...\Bronze\olist_customers_dataset.csv
2025-12-10 14:31:01 INFO Saved Silver parquet: C:\...\Silver\olist_customers_dataset.parquet
...
```

**Diret√≥rio criado:**
- `Datasets/Silver/` - Arquivos .parquet

#### **Etapa 3: Agrega√ß√£o (Silver ‚Üí Gold)**

```powershell
python .\src\gold.py
```

**Sa√≠da esperada:**
```
2025-12-10 14:32:00 INFO Saved avg_price_by_category
2025-12-10 14:32:00 INFO Saved price_variation_by_month
```

**Diret√≥rio criado:**
- `Datasets/Gold/` - KPIs em CSV

### 5. Valide os Resultados

```powershell
# Verifique estrutura de diret√≥rios
Get-ChildItem -Recurse .\Datasets\ | Select-Object FullName

# Verifique tamanho dos arquivos
ls -R .\Datasets\ | Format-Table Name, Length

# Inspecione KPIs gerados
Get-Content .\Datasets\Gold\avg_price_by_category.csv | Select-Object -First 10
Get-Content .\Datasets\Gold\price_variation_by_month.csv
```

### 6. Integre com BI Tool

#### **Power BI**

1. Abra Power BI Desktop
2. Home ‚Üí Get Data ‚Üí Folder
3. Aponte para `Datasets/Gold/`
4. Selecione arquivos CSV
5. Transform ‚Üí Load
6. Crie visualiza√ß√µes

#### **Metabase**

1. Inicie Metabase (Docker ou local)
2. Admin Settings ‚Üí Databases
3. Add Database ‚Üí File
4. Aponte para `Datasets/Gold/`
5. Browse ‚Üí Create Dashboard

### 7. Automa√ß√£o (Cron/Task Scheduler)

#### **Windows Task Scheduler**

```powershell
# Crie arquivo batch: run_pipeline.bat
@echo off
cd C:\Users\Administrador\TrabalhoBigData
.\venv\Scripts\Activate.ps1
python .\src\ingestao.py
python .\src\processamento.py
python .\src\gold.py
echo Pipeline executed at %date% %time% >> pipeline.log
```

Agenda no Task Scheduler:
- A√ß√£o: Executar `run_pipeline.bat`
- Frequ√™ncia: Di√°ria √†s 02:00

#### **Linux/macOS Cron**

```bash
# Edite crontab
crontab -e

# Adicione:
0 2 * * * cd /path/to/TrabalhoBigData && python src/ingestao.py && python src/processamento.py && python src/gold.py >> pipeline.log 2>&1
```

---

## Guia de Depend√™ncias

### Vers√µes M√≠nimas

```
Python                          3.8+
pandas                          1.3+
pyarrow (Parquet)              8.0+
fastparquet (Parquet alt.)     0.8+
pyspark (Opcional)             3.2+
```

### Instala√ß√£o Detalhada

```powershell
# 1. Upgrade pip
python -m pip install --upgrade pip

# 2. Instale requirements
pip install -r requirements.txt

# 3. Verifique vers√µes
pip show pandas pyarrow fastparquet

# 4. (Opcional) Instale PySpark para distribui√ß√£o
pip install pyspark==3.2.0
```

### Depend√™ncias de Sistema

| Componente | Sistema | Vers√£o |
|-----------|--------|--------|
| **Python** | Windows/Mac/Linux | 3.8+ |
| **Java** | Todos | 8+ (se usar PySpark) |
| **Docker** | Todos | 20.10+ (se usar MinIO) |

### MinIO (Opcional) - Docker Setup

```powershell
# Inicie MinIO
docker-compose -f .\infra\docker-compose.yaml up -d

# Acesse console
# URL: http://localhost:9001
# User: minioadmin
# Password: minioadmin

# Parar servi√ßo
docker-compose -f .\infra\docker-compose.yaml down
```

### Verifica√ß√£o de Compatibilidade

```python
# test_dependencies.py
import sys
import pandas as pd
import pyarrow as pa

print(f"Python: {sys.version}")
print(f"pandas: {pd.__version__}")
print(f"pyarrow: {pa.__version__}")

# Teste convers√£o Parquet
df = pd.DataFrame({'col1': [1, 2], 'col2': ['a', 'b']})
table = pa.Table.from_pandas(df)
print("Parquet support: OK")
```

---

## Descri√ß√£o dos Dados

### Origem dos Dados

**Fonte:** Olist Brazilian E-Commerce Public Dataset  
**URL:** https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce  
**Cobertura:** Pedidos de 2016-09 a 2018-10  
**Registros:** ~100k pedidos, ~1M linhas totais  
**Tamanho:** ~50-100 MB (dados brutos)  

### Tabelas/Datasets Principais

#### 1. **olist_orders_dataset.csv**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `order_id` | String | Identificador √∫nico do pedido |
| `customer_id` | String | ID do cliente |
| `order_status` | String | Status: delivered, cancelled, etc. |
| `order_purchase_timestamp` | DateTime | Quando o cliente fez a compra |
| `order_approved_at` | DateTime | Quando a compra foi aprovada |
| `order_delivered_carrier_date` | DateTime | Entrega pela transportadora |
| `order_delivered_customer_date` | DateTime | Entrega ao cliente |
| `order_estimated_delivery_date` | DateTime | Estimativa de entrega |

**Exemplo:**
```csv
order_id,customer_id,order_status,order_purchase_timestamp,...
e481f51cbdc54880170b7347a100c59b,9ef432eb6251297304e56dae2a56bf39,delivered,2016-09-04 21:15:13,...
```

#### 2. **olist_order_items_dataset.csv**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `order_id` | String | FK para orders |
| `order_item_id` | Int | Sequ√™ncia do item no pedido |
| `product_id` | String | FK para products |
| `seller_id` | String | FK para sellers |
| `shipping_limit_date` | DateTime | Limite para envio |
| `price` | Float | Pre√ßo unit√°rio do produto |
| `freight_value` | Float | Custo do frete |

**Exemplo:**
```csv
order_id,order_item_id,product_id,seller_id,price,freight_value
e481f51cbdc54880170b7347a100c59b,1,4244733047069,8e378e3d18f54efd,58.90,13.29
```

#### 3. **olist_products_dataset.csv**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `product_id` | String | Identificador √∫nico |
| `product_category_name` | String | Categoria do produto |
| `product_name_lenght` | Int | Comprimento do nome |
| `product_description_lenght` | Int | Comprimento da descri√ß√£o |
| `product_photos_qty` | Int | N√∫mero de fotos |
| `product_weight_g` | Float | Peso em gramas |
| `product_length_cm` | Float | Comprimento em cm |
| `product_height_cm` | Float | Altura em cm |
| `product_width_cm` | Float | Largura em cm |

**Exemplo:**
```csv
product_id,product_category_name,product_weight_g
4244733047069,telefonia,600
```

#### 4. **olist_customers_dataset.csv**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `customer_id` | String | Identificador √∫nico |
| `customer_zip_code_prefix` | String | CEP (5 d√≠gitos) |
| `customer_city` | String | Cidade |
| `customer_state` | String | UF (estado) |

#### 5. **olist_sellers_dataset.csv**

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `seller_id` | String | Identificador √∫nico |
| `seller_zip_code_prefix` | String | CEP (5 d√≠gitos) |
| `seller_city` | String | Cidade |
| `seller_state` | String | UF |

### Dicion√°rio de Dados por Camada

#### **RAW Layer**
- **Formato:** CSV
- **Modifica√ß√µes:** Nenhuma
- **Preserva√ß√£o:** 100%
- **Uso:** Auditoria, reprocessamento

#### **BRONZE Layer**
- **Formato:** CSV
- **Transforma√ß√µes:**
  - Normaliza√ß√£o de nomes de colunas (snake_case)
  - Sem altera√ß√£o de valores
- **Exemplo antes/depois:**
  ```
  Antes: "Product Category Name", "Product Weight (g)"
  Depois: product_category_name, product_weight_g
  ```

#### **SILVER Layer**
- **Formato:** Parquet
- **Transforma√ß√µes:**
  - Tipos de dados padronizados
  - Datas convertidas para datetime
  - Nulos em num√©ricos preenchidos com 0
  - Linhas: mesmo que Bronze (sem filtro)
- **Colunas:** Id√™nticas ao Bronze (nomes normalizados)

#### **GOLD Layer**

**Tabela 1: avg_price_by_category.csv**
```csv
product_category_name,avg_price
telefonica,150.45
electronics,120.30
books,25.00
```

**Tabela 2: price_variation_by_month.csv**
```csv
month,avg_price,min_price,max_price
2016-09,100.00,10.00,5000.00
2016-10,105.50,12.00,4800.00
```

### Estat√≠sticas do Dataset

| M√©trica | Valor |
|---------|-------|
| Total de Pedidos | ~100,000 |
| Total de Itens | ~1,000,000 |
| Per√≠odo | 2016-09 a 2018-10 |
| Categorias √önicas | ~70 |
| Vendedores | ~3,500 |
| Clientes | ~100,000 |
| Pre√ßo M√©dio | ~120 BRL |
| Pre√ßo M√≠n/M√°x | 0.85 / 13,664 BRL |

### Qualidade dos Dados Originais

| Problema | Frequ√™ncia | Impacto |
|----------|-----------|--------|
| Nomes de colunas inconsistentes | Alta | Bronze normaliza |
| Valores nulos em pre√ßo | Baixa | Silver preenche com 0 |
| Datas inv√°lidas | M√©dia | Silver converte ‚Üí NaT |
| Valores de pre√ßo 0 | Baixa | Question√°vel (desconto?) |
| Duplicatas de product_id | Rara | Mantidas (agrupa√ß√£o em Gold) |

---

## Pontos de Falha e Limita√ß√µes

### Falhas Operacionais

#### 1. **Arquivo Source N√£o Encontrado**

**Problema:**
```
WARNING: Source file not found: Datasets/Source/olist_customers_dataset.csv
```

**Causa:** Arquivo n√£o copiado ou estrutura de diret√≥rios incorreta

**Solu√ß√£o:**
- ‚úÖ Verificar se arquivos est√£o em `Datasets/Source/`
- ‚úÖ Usar argumentos `--source` e `--datasets-dir`:
  ```powershell
  python src/ingestao.py --source "C:\path\to\data" --datasets-dir "C:\path\to\Datasets"
  ```

#### 2. **Erro de Mem√≥ria (OutOfMemory)**

**Problema:** Dataset grande carregado completamente em RAM
```
MemoryError: Unable to allocate XXX MB
```

**Causa:** pandas carrega arquivo inteiro em mem√≥ria

**Solu√ß√£o:**
- Usar `pd.read_csv(chunksize=10000)` para arquivos > 1GB
- Migrar para PySpark (processamento distribu√≠do)
- Aumentar RAM da m√°quina

#### 3. **Colunas Duplicadas Ap√≥s Normaliza√ß√£o**

**Problema:**
```
"Price!" e "Price@" ‚Üí ambas normalizam para "price"
```

**Solu√ß√£o:**
- Adicionar counter a duplicatas:
  ```python
  if col in seen:
      col = f"{col}_{counter}"
  ```

#### 4. **NaT vs. 0 em Colunas Num√©ricas**

**Problema:** Pre√ßo 0 √© amb√≠guo (falta de dados? Promo√ß√£o?)

**Solu√ß√£o:**
- Documentar claramente a pol√≠tica
- Usar -1 ou NaN em vez de 0
- Valida√ß√µes espec√≠ficas por coluna

### Limita√ß√µes Arquiteturais

#### 1. **Processamento Sequencial (N√£o Paralelo)**

| Aspecto | Limita√ß√£o |
|--------|-----------|
| **Velocidade** | Etapas executadas em s√©rie (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold) |
| **Escalabilidade** | Dataset > 1GB pode ser lento em m√°quinas com <8GB RAM |
| **CPU** | N√£o aproveita m√∫ltiplos cores/CPUs |

**Impacto:**
- Pipeline 100k pedidos: ~2-5 minutos
- Pipeline 10M pedidos: ~1-2 horas

**Mitiga√ß√£o:**
```python
# Usar PySpark para paralelismo
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Pipeline").getOrCreate()
df = spark.read.csv("source.csv")
# Processamento distribu√≠do entre cores/n√≥s
```

#### 2. **Sem Orchestra√ß√£o/Scheduling**

**Problema:**
- Execu√ß√£o manual
- Sem retry autom√°tico em caso de falha
- Sem depend√™ncias entre tarefas

**Solu√ß√£o:**
- ‚úÖ Apache Airflow: DAGs, retry, monitoring
- ‚úÖ Prefect: Cloud-native, UI intuitiva
- ‚úÖ Cron/Task Scheduler: Simples para rotinas

#### 3. **Sem Data Quality Checks Complexos**

**Checagens Atuais:**
- ‚úì Arquivo existe?
- ‚úì Pode ler CSV?
- ‚úì Pode escrever?

**Checagens Ausentes:**
- ‚úó N√∫mero de linhas esperado?
- ‚úó Valores em range v√°lido (pre√ßo > 0)?
- ‚úó Chaves estrangeiras v√°lidas?
- ‚úó Outliers detectados?
- ‚úó Completude de dados (% n√£o-nulos)?

**Exemplo de melhoria:**
```python
def validate_prices(df):
    invalid = df[(df['price'] < 0) | (df['price'] > 100000)]
    if len(invalid) > 0:
        raise ValueError(f"Invalid prices: {len(invalid)} rows")
```

#### 4. **Sem Versionamento de Dados**

**Problema:** Ao executar pipeline novamente, dados anteriores s√£o sobrescritos

**Solu√ß√£o:**
```python
from datetime import datetime

run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
gold_dir = datasets_dir / f"Gold_{run_id}"
```

#### 5. **Sem Infraestrutura de Backup**

**Risco:** Perda de dados se disco falhar

**Solu√ß√£o:**
- MinIO (S3-compatible) em infra/
- Backup autom√°tico para cloud (AWS S3, Azure Blob)
- Replica√ß√£o para segundo disco

#### 6. **Sem API ou Exposi√ß√£o de Dados**

**Limita√ß√£o:** BI tools devem ler arquivos localmente

**Solu√ß√£o:**
- FastAPI para expor dados via HTTP
- GraphQL para queries customizadas
- Conex√£o direta SQL (Sqlite3 layer)

#### 7. **Normaliza√ß√£o de Colunas Destrutiva**

**Problema:**
```
Entrada:  "Produto (Categoria)"
Sa√≠da:    "produto_categoria"
Contexto: Par√™nteses perdidos
```

**Impacto:** Baixo (nomes apenas)

**Solu√ß√£o:** Manter mapping (original ‚Üí normalizado) em arquivo CSV

#### 8. **KPIs Gen√©ricos (N√£o Espec√≠ficos do Neg√≥cio)**

**KPIs Atuais:**
- avg_price_by_category
- price_variation_by_month

**An√°lises N√£o Cobertas:**
- Sazonalidade (Black Friday?)
- Vendedores com melhor performance
- Clientes com maior lifetime value
- Taxa de devolu√ß√£o por categoria
- Correla√ß√£o pre√ßo √ó avalia√ß√µes

**Solu√ß√£o:** Estender `gold.py` com KPIs adicionais

### Limita√ß√µes de Cobertura de Dados

#### 1. **Per√≠odo Hist√≥rico Fixo**

**Limita√ß√£o:** Dataset Olist √© est√°tico (2016-2018)

**Impacto:** N√£o reflete tend√™ncias recentes

**Solu√ß√£o:**
- Integra√ß√£o cont√≠nua com data provider (API)
- Append-only em Silver/Gold

#### 2. **Sem Dados Reais Atualizados**

**Problema:** Usando dataset p√∫blico; dados reais de empresa em branco

**Solu√ß√£o:**
- Substituir Source por pipeline real (DB, API)
- Manter mesma arquitetura

#### 3. **Sem Contexto Externo**

**Dados N√£o Inclu√≠dos:**
- Macroeconomia (infla√ß√£o, c√¢mbio)
- Concorr√™ncia (pre√ßos de rivais)
- Sazonalidade (feriados, eventos)

**Impacto:** KPIs descontextualizados

### Recomenda√ß√µes de Melhoria

| Prioridade | Item | Esfor√ßo | ROI |
|-----------|------|--------|-----|
| **P0** | Data Quality Framework | Alto | Alto |
| **P0** | Orchestra√ß√£o (Airflow) | Alto | Alto |
| **P1** | Versionamento de Dados | M√©dio | M√©dio |
| **P1** | Testes Automatizados | M√©dio | M√©dio |
| **P2** | KPIs Customizados | M√©dio | M√©dio |
| **P2** | Backup/DR | M√©dio | Alto |
| **P3** | API REST | Alto | Baixo |
| **P3** | Data Catalog | Alto | M√©dio |

---

## Ap√™ndice: Troubleshooting

### Problema: "ModuleNotFoundError: No module named 'pandas'"

```powershell
# Solu√ß√£o
pip install pandas>=1.3
```

### Problema: "No such file or directory: 'Datasets/Source'"

```powershell
# Solu√ß√£o
mkdir Datasets\Source
# Copie CSVs para este diret√≥rio
```

### Problema: Parquet files unreadable

```powershell
# Solu√ß√£o
pip install --upgrade pyarrow fastparquet
```

### Problema: OutOfMemory em datasets grandes

```powershell
# Use chunked reading
python src/processamento_fix.py  # vers√£o otimizada
```

### Problema: Timestamps n√£o parseiam corretamente

```python
# Verificar formato de data
df['order_purchase_timestamp'] = pd.to_datetime(
    df['order_purchase_timestamp'],
    format='%Y-%m-%d %H:%M:%S'  # formato expl√≠cito
)
```

---

## Conclus√£o

Este projeto implementa uma solu√ß√£o **end-to-end** de data lake com arquitetura medallion, processamento com Python/pandas e integra√ß√£o com ferramentas de BI. 

### Pontos Fortes
‚úÖ Arquitetura clara e escal√°vel  
‚úÖ Rastreabilidade completa (Raw ‚Üí Gold)  
‚úÖ Formatos otimizados (Parquet)  
‚úÖ C√≥digo documentado e modular  

### √Åreas de Melhoria
‚ö†Ô∏è Sem orchestra√ß√£o autom√°tica  
‚ö†Ô∏è Valida√ß√µes de qualidade limitadas  
‚ö†Ô∏è Sem backup/DR  
‚ö†Ô∏è KPIs gen√©ricos  

### Pr√≥ximos Passos
1. Validar com dados reais da empresa
2. Implementar Airflow para agendamento
3. Adicionar data quality checks
4. Estender com KPIs de neg√≥cio espec√≠ficos
5. Migrar para PySpark para escala

---

**√öltima Atualiza√ß√£o:** Dezembro 2025  
**Mantido por:** Projeto TrabalhoBigData
